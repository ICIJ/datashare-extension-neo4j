{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import dedupe\n",
    "\n",
    "dedupe_logger = logging.getLogger(dedupe.__name__)\n",
    "dedupe_logger.setLevel(logging.DEBUG)\n",
    "dedupe_logger.handlers = []\n",
    "dedupe_logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from neo4j_app import ROOT_DIR\n",
    "\n",
    "DATA_PATH = ROOT_DIR.joinpath(\"data\")\n",
    "records_path = DATA_PATH / \"person_records.csv\"\n",
    "excluded_set_path = DATA_PATH / \"excluded.txt\"\n",
    "clusters_path = DATA_PATH / \"person_clusters.json\"\n",
    "dedupe_path = DATA_PATH / \"person_deduped.csv\"\n",
    "training_set_path = DATA_PATH / \"person_training.csv\"\n",
    "graph_level_trained_model_path = DATA_PATH / \"graph_level_person_model.pickle\"\n",
    "graph_level_records_path = DATA_PATH / \"graph_level_person_records.csv\"\n",
    "graph_level_training_path = DATA_PATH / \"graph_level_person_training.json\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load a sample of the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from neo4j_app.constants import NE_MENTION_NORM\n",
    "from neo4j_app.ml.graph_dedupe import read_records\n",
    "\n",
    "with excluded_set_path.open() as f:\n",
    "    invalid_ids = (line.strip() for line in f)\n",
    "    invalid_ids = set(i for i in invalid_ids if i)\n",
    "\n",
    "with records_path.open() as f:\n",
    "    data = read_records(f, id_column=NE_MENTION_NORM, invalid_ids=invalid_ids)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from typing import Set\n",
    "from dedupe._typing import TrainingData\n",
    "from dedupe import read_training\n",
    "\n",
    "\n",
    "def filter_training_set(labeled_pairs: TrainingData, invalid: Set[str]) -> TrainingData:\n",
    "    labeled_pairs = deepcopy(labeled_pairs)\n",
    "    labeled_pairs[\"distinct\"] = [\n",
    "        (left, right)\n",
    "        for left, right in labeled_pairs[\"distinct\"]\n",
    "        if not left[NE_MENTION_NORM] in invalid\n",
    "        and not right[NE_MENTION_NORM] in invalid\n",
    "    ]\n",
    "    labeled_pairs[\"match\"] = [\n",
    "        (left, right)\n",
    "        for left, right in labeled_pairs[\"match\"]\n",
    "        if not left[NE_MENTION_NORM] in invalid\n",
    "        and not right[NE_MENTION_NORM] in invalid\n",
    "    ]\n",
    "    return labeled_pairs\n",
    "\n",
    "\n",
    "with training_set_path.open() as f:\n",
    "    training_set = filter_training_set(read_training(f), invalid=invalid_ids)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Downsample a bit the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "from dedupe._typing import Data\n",
    "\n",
    "\n",
    "def sample_data(dataset: Data, n_samples: int, sort_keys: Sequence[str]) -> Data:\n",
    "    samples = sorted(dataset.items(), key=lambda i: tuple((i[1][k] for k in sort_keys)))\n",
    "    return dict(samples[:n_samples])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Compute graph level features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from dedupe._typing import RecordDict, TrainingData\n",
    "from typing import Callable, Dict\n",
    "\n",
    "\n",
    "def add_mention_cluster_field(\n",
    "    record: RecordDict, clusters: Dict, *, id_field: str, cluster_field_name: str\n",
    ") -> RecordDict:\n",
    "    record = deepcopy(record)\n",
    "    record[cluster_field_name] = clusters[record[id_field]]\n",
    "    return record\n",
    "\n",
    "\n",
    "def augment_training_set(\n",
    "    labeled_pairs: TrainingData, augment_fn: Callable[[RecordDict], RecordDict]\n",
    ") -> TrainingData:\n",
    "    distinct = [\n",
    "        (augment_fn(lhs), augment_fn(rhs)) for lhs, rhs in labeled_pairs[\"distinct\"]\n",
    "    ]\n",
    "    match = [(augment_fn(lhs), augment_fn(rhs)) for lhs, rhs in labeled_pairs[\"match\"]]\n",
    "    training = TrainingData(distinct=distinct, match=match)\n",
    "    return training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "with dedupe_path.open() as f:\n",
    "    doc_level_clusters = read_records(\n",
    "        f, id_column=NE_MENTION_NORM, invalid_ids=invalid_ids\n",
    "    )\n",
    "doc_level_clusters = {\n",
    "    rec_id: rec[\"cluster_id\"] for rec_id, rec in doc_level_clusters.items()\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from neo4j_app.ml.graph_dedupe import NE_MENTION_CLUSTER\n",
    "\n",
    "data = {\n",
    "    rec_id: add_mention_cluster_field(\n",
    "        rec,\n",
    "        doc_level_clusters,\n",
    "        id_field=NE_MENTION_NORM,\n",
    "        cluster_field_name=NE_MENTION_CLUSTER,\n",
    "    )\n",
    "    for rec_id, rec in data.items()\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from neo4j_app.ml.graph_dedupe import NE_FIELDNAMES, write_dataset\n",
    "\n",
    "N_SAMPLES = None\n",
    "NEW_FIELDNAMES = NE_FIELDNAMES + [NE_MENTION_CLUSTER]\n",
    "if N_SAMPLES is not None:\n",
    "    data = sample_data(data, n_samples=N_SAMPLES, sort_keys=[\"docId\"])\n",
    "with graph_level_records_path.open(\"w\") as f:\n",
    "    write_dataset(data.values(), fieldnames=NEW_FIELDNAMES, dataset_f=f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import functools\n",
    "from dedupe import write_training\n",
    "\n",
    "if not graph_level_training_path.exists():\n",
    "    add_mention_cluster_field_fn = functools.partial(\n",
    "        add_mention_cluster_field,\n",
    "        clusters=doc_level_clusters,\n",
    "        id_field=NE_MENTION_NORM,\n",
    "        cluster_field_name=NE_MENTION_CLUSTER,\n",
    "    )\n",
    "    new_training_set = augment_training_set(training_set, add_mention_cluster_field_fn)\n",
    "    with graph_level_training_path.open(\"w\") as f:\n",
    "        write_training(new_training_set, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Make the most of the already annotated data\n",
    "\n",
    "Keep the training set, compute the new features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reading training from file\n",
      "Canopy: TfidfNGramCanopyPredicate: (0.8, mentionNorm)\n",
      "Canopy: TfidfNGramCanopyPredicate: (0.2, mentionNorm)\n",
      "Canopy: TfidfNGramCanopyPredicate: (0.6, mentionNorm)\n",
      "Canopy: TfidfNGramCanopyPredicate: (0.4, mentionNorm)\n",
      "Canopy: LevenshteinCanopyPredicate: (1, mentionNorm)\n",
      "Canopy: LevenshteinCanopyPredicate: (2, mentionNorm)\n",
      "Canopy: LevenshteinCanopyPredicate: (3, mentionNorm)\n",
      "Canopy: LevenshteinCanopyPredicate: (4, mentionNorm)\n",
      "Canopy: PartialIndexTfidfNGramCanopyPredicate: (0.2, mentionNorm, Surname)\n",
      "Canopy: PartialIndexTfidfNGramCanopyPredicate: (0.6, mentionNorm, CorporationName)\n",
      "Canopy: PartialIndexTfidfNGramCanopyPredicate: (0.2, mentionNorm, CorporationName)\n",
      "Canopy: PartialIndexTfidfNGramCanopyPredicate: (0.4, mentionNorm, CorporationName)\n",
      "Canopy: PartialIndexTfidfNGramCanopyPredicate: (0.8, mentionNorm, Surname)\n",
      "Canopy: PartialIndexTfidfNGramCanopyPredicate: (0.6, mentionNorm, Surname)\n",
      "Canopy: PartialIndexTfidfNGramCanopyPredicate: (0.4, mentionNorm, Surname)\n",
      "Canopy: PartialIndexTfidfNGramCanopyPredicate: (0.8, mentionNorm, CorporationName)\n",
      "Canopy: TfidfTextCanopyPredicate: (0.4, mentionNorm)\n",
      "Canopy: TfidfTextCanopyPredicate: (0.2, mentionNorm)\n",
      "Canopy: TfidfTextCanopyPredicate: (0.8, mentionNorm)\n",
      "Canopy: TfidfTextCanopyPredicate: (0.6, mentionNorm)\n",
      "Canopy: PartialIndexLevenshteinCanopyPredicate: (1, mentionNorm, CorporationName)\n",
      "Canopy: PartialIndexLevenshteinCanopyPredicate: (2, mentionNorm, Surname)\n",
      "Canopy: PartialIndexLevenshteinCanopyPredicate: (3, mentionNorm, CorporationName)\n",
      "Canopy: PartialIndexLevenshteinCanopyPredicate: (2, mentionNorm, CorporationName)\n",
      "Canopy: PartialIndexLevenshteinCanopyPredicate: (3, mentionNorm, Surname)\n",
      "Canopy: PartialIndexLevenshteinCanopyPredicate: (1, mentionNorm, Surname)\n",
      "Canopy: PartialIndexLevenshteinCanopyPredicate: (4, mentionNorm, Surname)\n",
      "Canopy: PartialIndexLevenshteinCanopyPredicate: (4, mentionNorm, CorporationName)\n",
      "Canopy: PartialIndexTfidfTextCanopyPredicate: (0.4, mentionNorm, Surname)\n",
      "Canopy: PartialIndexTfidfTextCanopyPredicate: (0.6, mentionNorm, CorporationName)\n",
      "Canopy: PartialIndexTfidfTextCanopyPredicate: (0.4, mentionNorm, CorporationName)\n",
      "Canopy: PartialIndexTfidfTextCanopyPredicate: (0.2, mentionNorm, Surname)\n",
      "Canopy: PartialIndexTfidfTextCanopyPredicate: (0.2, mentionNorm, CorporationName)\n",
      "Canopy: PartialIndexTfidfTextCanopyPredicate: (0.6, mentionNorm, Surname)\n",
      "Canopy: PartialIndexTfidfTextCanopyPredicate: (0.8, mentionNorm, CorporationName)\n",
      "Canopy: PartialIndexTfidfTextCanopyPredicate: (0.8, mentionNorm, Surname)\n",
      "Canopy: LevenshteinCanopyPredicate: (4, docDirname)\n",
      "Canopy: LevenshteinCanopyPredicate: (2, docDirname)\n",
      "Canopy: LevenshteinCanopyPredicate: (1, docDirname)\n",
      "Canopy: LevenshteinCanopyPredicate: (3, docDirname)\n",
      "Canopy: TfidfTextCanopyPredicate: (0.8, docDirname)\n",
      "Canopy: TfidfTextCanopyPredicate: (0.2, docDirname)\n",
      "Canopy: TfidfTextCanopyPredicate: (0.4, docDirname)\n",
      "Canopy: TfidfTextCanopyPredicate: (0.6, docDirname)\n",
      "Canopy: TfidfNGramCanopyPredicate: (0.8, mentionNorm)\n",
      "Canopy: TfidfNGramCanopyPredicate: (0.2, mentionNorm)\n",
      "Canopy: TfidfNGramCanopyPredicate: (0.6, mentionNorm)\n",
      "Canopy: TfidfNGramCanopyPredicate: (0.4, mentionNorm)\n",
      "Canopy: LevenshteinCanopyPredicate: (1, mentionNorm)\n",
      "Canopy: LevenshteinCanopyPredicate: (2, mentionNorm)\n",
      "Canopy: LevenshteinCanopyPredicate: (3, mentionNorm)\n",
      "Canopy: LevenshteinCanopyPredicate: (4, mentionNorm)\n",
      "Canopy: PartialIndexTfidfNGramCanopyPredicate: (0.2, mentionNorm, Surname)\n",
      "Canopy: PartialIndexTfidfNGramCanopyPredicate: (0.6, mentionNorm, CorporationName)\n",
      "Canopy: PartialIndexTfidfNGramCanopyPredicate: (0.2, mentionNorm, CorporationName)\n",
      "Canopy: PartialIndexTfidfNGramCanopyPredicate: (0.4, mentionNorm, CorporationName)\n",
      "Canopy: PartialIndexTfidfNGramCanopyPredicate: (0.8, mentionNorm, Surname)\n",
      "Canopy: PartialIndexTfidfNGramCanopyPredicate: (0.6, mentionNorm, Surname)\n",
      "Canopy: PartialIndexTfidfNGramCanopyPredicate: (0.4, mentionNorm, Surname)\n",
      "Canopy: PartialIndexTfidfNGramCanopyPredicate: (0.8, mentionNorm, CorporationName)\n",
      "Canopy: TfidfTextCanopyPredicate: (0.4, mentionNorm)\n",
      "Canopy: TfidfTextCanopyPredicate: (0.2, mentionNorm)\n",
      "Canopy: TfidfTextCanopyPredicate: (0.8, mentionNorm)\n",
      "Canopy: TfidfTextCanopyPredicate: (0.6, mentionNorm)\n",
      "Canopy: PartialIndexLevenshteinCanopyPredicate: (1, mentionNorm, CorporationName)\n",
      "Canopy: PartialIndexLevenshteinCanopyPredicate: (2, mentionNorm, Surname)\n",
      "Canopy: PartialIndexLevenshteinCanopyPredicate: (3, mentionNorm, CorporationName)\n",
      "Canopy: PartialIndexLevenshteinCanopyPredicate: (2, mentionNorm, CorporationName)\n",
      "Canopy: PartialIndexLevenshteinCanopyPredicate: (3, mentionNorm, Surname)\n",
      "Canopy: PartialIndexLevenshteinCanopyPredicate: (1, mentionNorm, Surname)\n",
      "Canopy: PartialIndexLevenshteinCanopyPredicate: (4, mentionNorm, Surname)\n",
      "Canopy: PartialIndexLevenshteinCanopyPredicate: (4, mentionNorm, CorporationName)\n",
      "Canopy: PartialIndexTfidfTextCanopyPredicate: (0.4, mentionNorm, Surname)\n",
      "Canopy: PartialIndexTfidfTextCanopyPredicate: (0.6, mentionNorm, CorporationName)\n",
      "Canopy: PartialIndexTfidfTextCanopyPredicate: (0.4, mentionNorm, CorporationName)\n",
      "Canopy: PartialIndexTfidfTextCanopyPredicate: (0.2, mentionNorm, Surname)\n",
      "Canopy: PartialIndexTfidfTextCanopyPredicate: (0.2, mentionNorm, CorporationName)\n",
      "Canopy: PartialIndexTfidfTextCanopyPredicate: (0.6, mentionNorm, Surname)\n",
      "Canopy: PartialIndexTfidfTextCanopyPredicate: (0.8, mentionNorm, CorporationName)\n",
      "Canopy: PartialIndexTfidfTextCanopyPredicate: (0.8, mentionNorm, Surname)\n",
      "Canopy: LevenshteinCanopyPredicate: (4, docDirname)\n",
      "Canopy: LevenshteinCanopyPredicate: (2, docDirname)\n",
      "Canopy: LevenshteinCanopyPredicate: (1, docDirname)\n",
      "Canopy: LevenshteinCanopyPredicate: (3, docDirname)\n",
      "Canopy: TfidfTextCanopyPredicate: (0.8, docDirname)\n",
      "Canopy: TfidfTextCanopyPredicate: (0.2, docDirname)\n",
      "Canopy: TfidfTextCanopyPredicate: (0.4, docDirname)\n",
      "Canopy: TfidfTextCanopyPredicate: (0.6, docDirname)\n"
     ]
    }
   ],
   "source": [
    "from dedupe import Dedupe\n",
    "import functools\n",
    "from neo4j_app.ml.graph_dedupe import (\n",
    "    ConfigurableClassifierDedupe,\n",
    "    person_fields,\n",
    "    run_training,\n",
    ")\n",
    "\n",
    "# TODO: increase\n",
    "training_sample_size = 50000\n",
    "target_recall = 0.8\n",
    "\n",
    "clf_args = {\"max_iter\": 100000}\n",
    "model = run_training(\n",
    "    graph_level_records_path,\n",
    "    dedupe_getter=functools.partial(ConfigurableClassifierDedupe, clf_args=clf_args),\n",
    "    fields_getter=functools.partial(person_fields, inside_docs=False),\n",
    "    excluded_path=excluded_set_path,\n",
    "    model_path=graph_level_trained_model_path,\n",
    "    training_path=graph_level_training_path,\n",
    "    sample_size=training_sample_size,\n",
    "    id_column=NE_MENTION_NORM,\n",
    "    recall=target_recall,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

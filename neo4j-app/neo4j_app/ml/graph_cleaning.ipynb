{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Load data\n",
    "\n",
    "We restrict the data by entity type for now"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import dedupe\n",
    "\n",
    "dedupe_logger = logging.getLogger(dedupe.__name__)\n",
    "dedupe_logger.setLevel(logging.DEBUG)\n",
    "dedupe_logger.handlers = []\n",
    "dedupe_logger.addHandler(logging.StreamHandler())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import neo4j\n",
    "\n",
    "neo4j_driver = neo4j.AsyncGraphDatabase.driver(\"neo4j://localhost:7687\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "DATASHARE_BASE_URL = \"http://localhost:8080\"\n",
    "DATASHARE_PROJECT_URL = f\"{DATASHARE_BASE_URL}/#/d/local-datashare\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from typing import AsyncGenerator, Optional\n",
    "\n",
    "\n",
    "async def retrieve_ne(\n",
    "        session: neo4j.AsyncSession,\n",
    "        ne_category: str,\n",
    "        *,\n",
    "        limit: Optional[int] = None,\n",
    ") -> AsyncGenerator[neo4j.Record, None]:\n",
    "    query = f\"\"\"MATCH (ne:NamedEntity:{ne_category})-[rel]->(doc:Document)\n",
    "OPTIONAL MATCH (doc)-[HAS_PARENT]->(rootDoc:Document)\n",
    "RETURN ne, doc, rootDoc\n",
    "\"\"\"\n",
    "    if limit:\n",
    "        query = f\"\"\"{query}\n",
    "LIMIT {limit}\n",
    "\"\"\"\n",
    "    res = await session.run(query)\n",
    "    async for rec in res:\n",
    "        yield rec"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import itertools\n",
    "import string\n",
    "from neo4j_app.core.utils.pydantic import to_lower_camel\n",
    "from neo4j_app.constants import (\n",
    "    DOC_CONTENT_TYPE,\n",
    "    DOC_DIRNAME,\n",
    "    DOC_ID,\n",
    "    DOC_PATH,\n",
    "    DOC_ROOT_ID,\n",
    "    NE_MENTION_NORM,\n",
    ")\n",
    "from typing import Dict, Optional\n",
    "\n",
    "NE_DOC_ID = to_lower_camel(f\"doc_{DOC_ID}\")\n",
    "NE_DOC_DIR_NAME = to_lower_camel(f\"doc_{DOC_DIRNAME}\")\n",
    "NE_DOC_FILENAME = \"docFilename\"\n",
    "NE_DOC_CONTENT_TYPE = to_lower_camel(f\"doc_{DOC_CONTENT_TYPE}\")\n",
    "NE_DOC_ROOT_ID = to_lower_camel(f\"doc_{DOC_ROOT_ID}\")\n",
    "NE_DEBUG_DOC_URL = \"debugDocUrl\"\n",
    "NE_DEBUG_FILENAME = \"debugFilename\"\n",
    "\n",
    "NE_FIELDNAMES = [\n",
    "    NE_MENTION_NORM,\n",
    "    NE_DOC_ID,\n",
    "    NE_DOC_DIR_NAME,\n",
    "    NE_DOC_FILENAME,\n",
    "    NE_DOC_CONTENT_TYPE,\n",
    "    NE_DOC_ROOT_ID,\n",
    "    NE_DEBUG_DOC_URL,\n",
    "    NE_DEBUG_FILENAME,\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "_TRANSLATE_PUNCT = str.maketrans(dict(zip(string.punctuation, itertools.repeat(\" \"))))\n",
    "\n",
    "\n",
    "# TODO: refine this violent preprocessing...\n",
    "\n",
    "\n",
    "def _replace_double_white_spaces(s: str) -> str:\n",
    "    while \"  \" in s:\n",
    "        s = s.replace(\"  \", \" \")\n",
    "    return s\n",
    "\n",
    "\n",
    "def preprocess_filename(filename: str) -> Optional[str]:\n",
    "    filename = filename.translate(_TRANSLATE_PUNCT)\n",
    "    filename = _replace_double_white_spaces(filename)\n",
    "    if not filename:\n",
    "        filename = None\n",
    "    filename = filename.lower().strip()\n",
    "    return filename\n",
    "\n",
    "\n",
    "def preprocess_dirname(dirname: str) -> str:\n",
    "    dirname = \" \".join(item for item in dirname.split(\"/\") if item)\n",
    "    return dirname\n",
    "\n",
    "\n",
    "def doc_url(project_url: str, *, doc_id: str, root_id: Optional[str]) -> str:\n",
    "    return f\"{project_url}/{root_id if root_id is not None else doc_id}/{doc_id}\"\n",
    "\n",
    "\n",
    "def neo4j_to_record(record: neo4j.Record, project_url: str) -> Dict:\n",
    "    rec = dict()\n",
    "    ne = record[\"ne\"]\n",
    "    rec[NE_MENTION_NORM] = ne[NE_MENTION_NORM]\n",
    "    doc = record[\"doc\"]\n",
    "    rec[NE_DOC_ID] = doc[DOC_ID]\n",
    "    rec[NE_DOC_DIR_NAME] = preprocess_dirname(doc[DOC_DIRNAME])\n",
    "    raw_filename = doc[DOC_PATH].split(\"/\")[-1]\n",
    "    rec[NE_DOC_FILENAME] = preprocess_filename(raw_filename)\n",
    "    rec[NE_DOC_CONTENT_TYPE] = doc[DOC_CONTENT_TYPE]\n",
    "    root_doc = record[\"rootDoc\"]\n",
    "    root_id = None\n",
    "    if root_doc is not None:\n",
    "        root_id = root_doc[DOC_ID]\n",
    "    rec[NE_DOC_ROOT_ID] = root_id\n",
    "    # Debug\n",
    "    rec[NE_DEBUG_DOC_URL] = doc_url(\n",
    "        project_url, doc_id=rec[NE_DOC_ID], root_id=rec[NE_DOC_ROOT_ID]\n",
    "    )\n",
    "    rec[NE_DEBUG_FILENAME] = raw_filename\n",
    "    return rec"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import csv\n",
    "from typing import AsyncIterable, Iterable, List, TextIO\n",
    "\n",
    "\n",
    "async def write_dataset(\n",
    "        records: AsyncIterable[Dict], fieldnames: List[str], dataset_f: TextIO\n",
    "):\n",
    "    writer = csv.DictWriter(dataset_f, fieldnames)\n",
    "    writer.writeheader()\n",
    "    async for rec in records:\n",
    "        writer.writerow(rec)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from neo4j_app import ROOT_DIR\n",
    "\n",
    "DATA_PATH = ROOT_DIR.joinpath(\"data\")\n",
    "records_path = DATA_PATH / \"person_records.csv\"\n",
    "training_set_path = DATA_PATH / \"person_training.csv\"\n",
    "trained_model_path = DATA_PATH / \"person_model.pickle\"\n",
    "clusters_path = DATA_PATH / \"person_clusters.csv\"\n",
    "excluded_set_path = DATA_PATH / \"excluded.txt\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# TODO: remove the limit of training data\n",
    "NUM_SAMPLES = None\n",
    "\n",
    "async with neo4j_driver.session() as sess:\n",
    "    if not records_path.exists():\n",
    "        with records_path.open(\"w\") as df:\n",
    "            recs = (\n",
    "                neo4j_to_record(rec, project_url=DATASHARE_PROJECT_URL)\n",
    "                async for rec in retrieve_ne(\n",
    "                sess, ne_category=\"PERSON\", limit=NUM_SAMPLES\n",
    "            )\n",
    "            )\n",
    "            await write_dataset(recs, fieldnames=NE_FIELDNAMES, dataset_f=df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def person_fields(\n",
    "        *,\n",
    "        all_mentions: Iterable[str],\n",
    "        all_dirnames: Iterable[str],\n",
    "        all_categories: Iterable[str],\n",
    "        inside_docs: bool,\n",
    ") -> List[Dict]:\n",
    "    # TODO: add the number of documents of the same type in the doc as feature ?\n",
    "    fields = [\n",
    "        # Use both string and text type for mention norm to capture char-level (String)\n",
    "        # and words (Text) similarities and differences\n",
    "        {\"field\": NE_MENTION_NORM, \"type\": \"String\"},\n",
    "        {\"field\": NE_MENTION_NORM, \"type\": \"Text\", \"corpus\": all_mentions},\n",
    "        #\n",
    "        {\"field\": NE_MENTION_NORM, \"type\": \"Name\"},\n",
    "    ]\n",
    "    if not inside_docs:\n",
    "        cross_doc_fields = [\n",
    "            # Exact match on IDs\n",
    "            {\"field\": NE_DOC_ID, \"type\": \"Exact\"},\n",
    "            {\"field\": NE_DOC_ROOT_ID, \"type\": \"Exact\", \"has missing\": True},\n",
    "            # Finite set of values for categories\n",
    "            {\n",
    "                \"field\": NE_DOC_CONTENT_TYPE,\n",
    "                \"type\": \"Categorical\",\n",
    "                \"categories\": all_categories,\n",
    "            },\n",
    "            # We hope that some file names will have word level similarities\n",
    "            {\"field\": NE_DOC_FILENAME, \"type\": \"Exact\"},\n",
    "            # We hope that some file names will have word level similarities\n",
    "            {\"field\": NE_DOC_DIR_NAME, \"type\": \"Text\", \"corpus\": all_dirnames},\n",
    "        ]\n",
    "        fields.extend(cross_doc_fields)\n",
    "    return fields"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from typing import Set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def get_filenames(records: Iterable[Dict]) -> Set[str]:\n",
    "    filenames = set(rec[NE_DOC_FILENAME] for rec in records)\n",
    "    return filenames"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def get_mentions(records: Iterable[Dict]) -> Set[str]:\n",
    "    filenames = set(rec[NE_MENTION_NORM] for rec in records)\n",
    "    return filenames"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def get_dirnames(records: Iterable[Dict]) -> Set[str]:\n",
    "    dirname = set(rec[NE_DOC_DIR_NAME] for rec in records)\n",
    "    return dirname"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def get_categories(records: Iterable[Dict]) -> Set[str]:\n",
    "    dirname = set(rec[NE_DOC_CONTENT_TYPE] for rec in records)\n",
    "    return dirname"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "from neo4j_app.ml.utils import filtering_console_label\n",
    "from neo4j_app.ml.graph_dedupe import DocumentGraphDedupe, read_records\n",
    "from contextlib import contextmanager\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def yield_none():\n",
    "    yield None\n",
    "\n",
    "\n",
    "def run_training(\n",
    "        data_path: Path,\n",
    "        *,\n",
    "        excluded_path: Path,\n",
    "        model_path: Path,\n",
    "        training_path: Path,\n",
    "        sample_size: int,\n",
    "        doc_id_column: str,\n",
    "        id_column: str,\n",
    "        recall: float,\n",
    ") -> dedupe.Dedupe:\n",
    "    with excluded_path.open() as f:\n",
    "        invalid_ids = (line.strip() for line in f)\n",
    "        invalid_ids = set(i for i in invalid_ids if i)\n",
    "    with data_path.open() as f:\n",
    "        records = read_records(f, id_column=id_column, invalid_ids=invalid_ids)\n",
    "\n",
    "    field = person_fields(\n",
    "        all_mentions=get_mentions(records.values()),\n",
    "        all_dirnames=get_dirnames(records.values()),\n",
    "        all_categories=get_categories(records.values()),\n",
    "        inside_docs=True,\n",
    "    )\n",
    "    # TODO: we should modify dedupe to be able to pass arguments to the\n",
    "    #  classifier, here max_iter is too low to allow for convergence\n",
    "    deduper = DocumentGraphDedupe(doc_key=doc_id_column, variable_definition=field)\n",
    "\n",
    "    training_file_cm = yield_none\n",
    "    if training_path.exists():\n",
    "        training_file_cm = training_path.open\n",
    "    with training_file_cm() as training_file:\n",
    "        deduper.prepare_training(records, training_file, sample_size)\n",
    "\n",
    "    invalid = filtering_console_label(deduper, id_column=id_column)\n",
    "    invalid_ids.update((rec[id_column] for rec in invalid))\n",
    "    excluded_path.write_text(\"\\n\".join(invalid_ids))\n",
    "\n",
    "    with training_path.open(\"w\") as f:\n",
    "        deduper.write_training(f)\n",
    "\n",
    "    deduper.train(recall=recall)\n",
    "    with model_path.open(\"wb\") as f:\n",
    "        deduper.write_settings(f)\n",
    "\n",
    "    return deduper"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: increase\n",
    "training_sample_size = 50000\n",
    "target_recall = 0.8\n",
    "# TODO: increase\n",
    "model = run_training(\n",
    "    records_path,\n",
    "    excluded_path=excluded_set_path,\n",
    "    model_path=trained_model_path,\n",
    "    training_path=training_set_path,\n",
    "    sample_size=training_sample_size,\n",
    "    doc_id_column=NE_DOC_ID,\n",
    "    id_column=NE_MENTION_NORM,\n",
    "    recall=target_recall,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
